Contents in the Readme file below:
Pipeline
Data Preprocessing
Importance of Data Preprocessing
Technique used to preprocess
Why these techniques are used?
Why other techniques are not used?
What other optimization can be done?


Training of the Model
Why GPT2 is used
What other transformers can be used?

Evalution Metrices
Types of Metrices can be used
What are importance of different types of metrics?

Testing on the examples
Improvement and Conclusion
Problems I faced
What other approaches can be used?
Sources used in the Assignment

1. Pipeline of the model

Data Preprocessing
Model Training using GPT2 Transformer
Evaluation of the model
Interaction



2. Data Preprocessing

Importance of Data Preprocessing

Text preprocessing is a crucial step in Natural Language Processing (NLP) that involves cleaning and transforming raw text data into a format suitable for analysis and machine learning models. This process is vital for enhancing the performance and accuracy of NLP tasks.

One key reason for text preprocessing is to remove noise and irrelevant information from the text, such as special characters, punctuation, and stop words. This helps in reducing the dimensionality of the data and improves the efficiency of subsequent analysis. Additionally, text normalization techniques, such as stemming and lemmatization, ensure that words are represented in their base or root form, reducing redundancy and enhancing the consistency of the dataset.


Techniques used in my submission

Lowecasing : Lowercasing text in NLP preprocessing involves converting all letters in a text to lowercase. This step is essential for standardizing text data because it treats words with different cases (e.g., "Word" and "word") as the same, reducing vocabulary size and improving model efficiency. It ensures consistency in word representations, making it easier for algorithms to recognize patterns and associations This normalization simplifies subsequent processing steps, such as tokenization and feature extraction. Even though we have not done feature extraction  in our model but its still useful
Removal of HTML Tags : Removing HTML tags is an essential step in NLP text preprocessing to ensure that only meaningful textual content is analyzed. HTML tags contain formatting information and metadata irrelevant to linguistic analysis. Including these tags can introduce noise and distort the analysis results
 Remove URLS : Removing HTML tags is an essential step in NLP text preprocessing to ensure that only meaningful textual content is analyzed. HTML tags contain formatting information and metadata irrelevant to linguistic analysis. Its useful technique, even though our data does not have URLS.
 Remove Punctuation : Removing punctuation marks is essential in NLP text preprocessing to enhance the accuracy and efficiency of analysis. Punctuation marks like commas, periods, and quotation marks carry little semantic meaning and can introduce noise into the dataset. By removing them, the text becomes cleaner and more uniform, making it easier for machine learning models to extract meaningful features and patterns.
Remove Chatwords : Handling ChatWords, also known as internet slang or informal language used in online communication, is important in NLP text preprocessing to ensure accurate analysis and understanding of text data. By converting ChatWords into their standard English equivalents or formal language equivalents, NLP models can effectively interpret the meaning of the text.
There are also no chatwords in our dataset as observed manually.
Missing values : No missing values in the data, so not applicable.

Spell Correction: Spelling correction is a crucial aspect of NLP text preprocessing to enhance data quality and improve model performance. It addresses errors in text caused by typographical mistakes, irregularities, or variations in spelling.

Though this data, seemed generated by a computer, it does not have any spelling errors. It takes a lot of time to apply it so has been avoided.

Removing Stopwords (Important) : In NLP text preprocessing, removing stop words is crucial enhancingnce the quality and efficiency of the analysis. Stop words are common words like "the," "is," and "and," which appear frequently in text but carry little semantic meaning. By eliminating stop words, we reduce noise in the data, decrease the dimensionality of the dataset, and improve the accuracy of NLP tasks such as sentiment analysis, topic modeling, and text classification. This process streamlines the analysis by focusing on the significant words that carry more meaningful information, leading to better model performance and interpretation of results. 
Handling Emojis : No emojis in the dataset.

    10. Stemming(Important) : Stemming is a text preprocessing technique in NLP used to reduce words to their root or base form, known as a stem, by removing suffixes. It helps in simplifying the vocabulary and reducing word variations, thereby improving the efficiency of downstream NLP tasks like information retrieval and sentiment analysis. By converting words to their common root, stemming increases the overlap between related words, enhancing the generalization ability of models. 


3. Training of the Model


Why use GPT2?

 GPT-2 is a good transformer for text-to-text generation due to its exceptional capabilities and features:

Natural Language Understanding and Generation: GPT excels at understanding and generating human-like text. Its ability to maintain context over long passages makes it suitable for tasks that require generating detailed and contextually appropriate responses.

Transformer Architecture: The transformer architecture, with its self-attention mechanism, allows GPT models to capture long-range dependencies and relationships in text more effectively than earlier models like RNNs and LSTMs(important).

Pretrained Knowledge: It has been trained on a diverse dataset containing a vast amount of internet text, which allows it to understand and generate human-like text across various topics and styles.

Versatility(Important): GPT-2 can be fine-tuned for specific tasks, making it adaptable for various text generation applications, including storytelling, summarization, translation, and more. It is good for text- to text generation.


Ease of Use: With pre-built libraries and tools available (e.g., Hugging Face's Transformers library), integrating GPT-2 into projects is straightforward, enabling quick implementation and experimentation.



What other Transformers can be used?

	There are several other transformer models that can be used for text generation and various natural language processing (NLP) tasks. Here are some notable alternatives to GPT:

BERT (Bidirectional Encoder Representations from Transformers):

Developer: Google
Strengths: BERT is designed for understanding the context of a word in search queries. It excels in tasks like question answering, named entity recognition, and sentiment analysis. (It is expected to give similar results according to me.)
RoBERTa (A Robustly Optimized BERT Pretraining Approach):

Developer: Facebook AI
Strengths: RoBERTa is an optimized version of BERT, trained with more data and computational resources. It often outperforms BERT on many benchmarks. It is indeed great, but GPT2 is a little easy to use than this.

T5 (Text-To-Text Transfer Transformer):

Developer: Google
Strengths: T5 frames all NLP tasks as a text-to-text problem, enabling a unified approach to various tasks such as translation, summarization, and question answering.

Problem in T5: It is generally used for Machine Translation and not text generation, so avoided it.


Note :  There are also other transformers like XLBert, DistilBert, GPT3,Electra etc.  but these above 3 are popular the most and popular to use.



3. Evaluation 


Evaluation Metrics for Text-to-Text Generation

BLEU (Bilingual Evaluation Understudy)

Description: Measures n-gram overlap between generated and reference texts, up to 4-grams.
Advantages: Widely accepted, good for exact match tasks like translation.
Disadvantages: May not align well with human judgment for creative tasks; can be harsh on phrasing variations.
ROUGE (Recall-Oriented Understudy for Gisting Evaluation)

Description: Measures recall using n-gram, longest common subsequence, or word sequences overlap.
Advantages: Effective for summarization; multiple variants for different text aspects.
Disadvantages: May not fully capture creative text quality; high recall doesn’t always mean better text.
Perplexity

Description: Evaluates model’s prediction capability; lower perplexity indicates better prediction.
Advantages: Monitors model consistency and confidence during training.
Disadvantages: Doesn’t measure text quality directly; low perplexity doesn’t always equal better human evaluation.
Human Evaluation

Description: Human judges assess fluency, coherence, relevance, and creativity.
Advantages: Reflects actual human-perceived quality; captures nuances.
Disadvantages: Time-consuming, costly, and subjective.
METEOR (Metric for Evaluation of Translation with Explicit ORdering)

Description: Uses synonyms, stemming, and paraphrasing for alignment; calculates precision and recall.
Advantages: More flexible than BLEU; better human judgment correlation.
Disadvantages: Computationally intensive; still reference-dependent.





Interaction Examples

There are 3 examples I used. These examples were trained with 1 epoch and the results are below

 



Improvements and Conclusion:
The results are not very good because it was only trained on one epoch, but it gives the result anyway.
 I can improve the result with increased no of epochs, but need a better device to run the code. (my system is i5 and I ran this on a Kaggle notebook with T4 GPU). 
The memory error has been avoided by using clearing the cache. It worked sometimes, but not every time because the dataset is VERY huge.


How can the results be improved according to me ?
Using LLMs APIs, they will give great results, but not used as prohibited.
Increasing number of epochs. Limited time of the assignment, could not allow me use more epochs. 
Using a more complex Transformer such as GPT3, but it requires high computational power.
Data Augmentation: This will increase the dataset, if overfitting is controlled, it can increase the accuracy of the model. But again, its the computation that prohibited me from using it.
Regularization techniques : Adding dropouts, L1, L2 regularization can help reduce overfitting.
Post Processing : Refine the generated text using grammatical correction tools, rephrasing, or additional filtering.
Hyperparameter Tuning (Of course) : changing epochs, batch size, learning rate can change the results and we can optimize accordingly.
But it will take time to optimize and limited time did not allow me to do it.
	

6. Problems I faced
 I tried first training on the dataset on 1 epoch to check the results, it worked and produced results, not good though, but yes they were produced.
I tried hyperparameter tuning by increasing the number of epochs but due to internet issue, my model could not trained as seen in the kaggle notebook.
Another problem, I faced was, when I trained on the whole dataset, it produces results, but splitting the dataset into val, and test, drastically produced very bad results.
The reason I see behind is this can be the Data Leakage since there were many similar rows. But my model failed to train on val and test set on large epochs(such as 3) so could not calculate the accuracy metrics. But the loss was calculated anyway on (0.5 epochs, yes very less).
Low Time and High computation power bounded the results, a better device and more time can help alter the training in a good way.


7. Other Approach

I initially thought of using LSTMs but due to very large dataset, I avoided it. High Computation limits this approach, if used it can give good results, but it is very slow compared to using a transformer which is pretrained and fast.
Also, LSTMs will give better results according to me, since it will fit on the dataset from scratch.
Transformer with fine tuning is the best approach possible considering time and computational cost.



8. Sources used

This code is mostly written by me, but online open sources are taken into consideration.
Also for the readme file, some of the texts has been produced by ChatGPT, though it has been verified and presented in the most natural way possible by me. It has been fine tuned by me, so this readme file looks manual as possible.


Thank You, It was a great project working on.


	









	










